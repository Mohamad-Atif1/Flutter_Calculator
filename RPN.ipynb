{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohamad-Atif1/Flutter_Calculator/blob/master/RPN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UvmkAkM5RKNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HERE WE START"
      ],
      "metadata": {
        "id": "9530ViRoLzBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pycocotools.coco import COCO\n",
        "import numpy as np\n",
        "import skimage.io as io\n",
        "import matplotlib.pyplot as plt\n",
        "# import pylab\n",
        "# pylab.rcParams['figure.figsize'] = (8.0, 10.0) # i need to understand thos\n"
      ],
      "metadata": {
        "id": "Sit4Lu2RNe0g"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sN-fKa9q9qPu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lfaYSu53PSc",
        "outputId": "4b0cbd74-9cb8-45dc-84f4-bca3a1e39634"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataDir='/content/drive/MyDrive/cocoDataset'\n",
        "dataType='val2017'\n",
        "annFile='{}/instances_{}.json'.format(dataDir,dataType)\n",
        "NEWSIZE = (360,360)\n",
        "BATCH_SIZE = 8"
      ],
      "metadata": {
        "id": "vG3W-LAw69fk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coco=COCO(annFile)\n",
        "img_idx = coco.getImgIds()[555]\n",
        "image = coco.loadImgs(img_idx)\n",
        "len(io.imread(image[0]['coco_url']))\n",
        "\n",
        "\n",
        "image[0]['coco_url']"
      ],
      "metadata": {
        "id": "AoAyozuheFru",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "eb1832a2-b181-45b8-c803-3f999c5ef003"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=3.22s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'http://images.cocodataset.org/val2017/000000552371.jpg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# rpn.RegionProposalNetwork\n",
        "aaa = []"
      ],
      "metadata": {
        "id": "5ojlF-3pZR54"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x,y,width,height = box\n",
        "# x2 = x + width\n",
        "# y2 = y + height\n",
        "\n",
        "# x = x * wScale / self.image[1]\n",
        "# x2 = x2 * wScale /self.image[1]\n",
        "# y = y *  hScale /self.image[0]\n",
        "#  y2 = y2 * hScale /self.image[0]"
      ],
      "metadata": {
        "id": "YCdksIXJ85RN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomeDataset(Dataset):\n",
        "\n",
        "    def __init__(self,path,trans=None):\n",
        "        super().__init__()\n",
        "        self.coco = COCO(path)\n",
        "        self.trans = trans\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(coco.getImgIds())\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        img_idx = self.coco.getImgIds()[idx]\n",
        "        self.image = coco.loadImgs(img_idx)\n",
        "        self.image = io.imread(self.image[0]['coco_url'])\n",
        "\n",
        "        hScale = NEWSIZE[0]/self.image.shape[0]\n",
        "        wScale = NEWSIZE[1]/self.image.shape[1]\n",
        "\n",
        "        if self.trans:\n",
        "            self.image = self.trans(self.image)\n",
        "\n",
        "\n",
        "        annotations_idx = self.coco.getAnnIds(imgIds=img_idx)\n",
        "        annotations = self.coco.loadAnns(annotations_idx)\n",
        "        bbox = [self.resize_box(wScale,hScale,annotation['bbox']) for annotation in annotations ]\n",
        "\n",
        "        # bbox = [annotation['bbox'] for annotation in annotations ]\n",
        "\n",
        "        bbox = torch.tensor(bbox)\n",
        "        bbox = bbox.to(device)\n",
        "        return (self.image,bbox)\n",
        "\n",
        "\n",
        "\n",
        "    def resize_box(self,wScale,hScale,box):\n",
        "        # print(type(box),'============================')\n",
        "        x,y,width,height = box\n",
        "        x2 = x + width\n",
        "        y2 = y + height\n",
        "\n",
        "        x = x * wScale / NEWSIZE[1]\n",
        "        x2 = x2 * wScale /NEWSIZE[1]\n",
        "        y = y *  hScale /NEWSIZE[0]\n",
        "        y2 = y2 * hScale /NEWSIZE[0]\n",
        "\n",
        "        width = width * wScale\n",
        "        height =  height * hScale\n",
        "\n",
        "        return [x,y,x2,y2]\n"
      ],
      "metadata": {
        "id": "86EbUDyndXtb"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(batch):\n",
        "    images = []\n",
        "    targets = []\n",
        "\n",
        "    for image, target in batch:\n",
        "        images.append(image)\n",
        "        dict_tragets = {}\n",
        "        dict_tragets['boxes'] = target\n",
        "        dict_tragets['labels'] = torch.ones(len(target),dtype=torch.int64)\n",
        "        targets.append(dict_tragets)\n",
        "\n",
        "\n",
        "    # Stack images into a batch\n",
        "    images = torch.stack(images, dim=0)\n",
        "    # targets = torch.stack(targets,dim=0)  i cannot do this because image comes with different number of boxes\n",
        "    #\n",
        "\n",
        "    return images,targets"
      ],
      "metadata": {
        "id": "2QK6SGs1ByNU"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "trans = transforms.Compose([\n",
        "   transforms.ToTensor(),\n",
        "   transforms.Resize(NEWSIZE),\n",
        "#    transforms.CenterCrop(224),\n",
        "#    transforms.Normalize(mean=[0.485, 0.456, 0.406], # ImageNet\n",
        "#                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "dataset = CustomeDataset(path=annFile,trans=trans)\n",
        "\n",
        "train_loader = DataLoader(dataset,batch_size=BATCH_SIZE,collate_fn = custom_collate_fn)\n",
        "dataset[8][1]"
      ],
      "metadata": {
        "id": "xLWZvWlhgJxt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "886fd6d9-05a0-4356-a61f-e3b091bc79b8"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.44s)\n",
            "creating index...\n",
            "index created!\n",
            "cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0520, 0.0000, 0.5257, 1.0000],\n",
              "        [0.3575, 0.4046, 0.6626, 0.8294],\n",
              "        [0.3693, 0.0271, 0.7892, 0.9174],\n",
              "        [0.5418, 0.1727, 0.8447, 0.7354],\n",
              "        [0.7006, 0.0973, 0.9166, 0.4852],\n",
              "        [0.6248, 0.0667, 0.8600, 0.5660],\n",
              "        [0.8437, 0.1852, 1.0000, 0.5397],\n",
              "        [0.0000, 0.0000, 0.3152, 1.0000],\n",
              "        [0.7879, 0.3190, 0.8701, 0.3994],\n",
              "        [0.6697, 0.3036, 0.8802, 0.5476]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_loader))"
      ],
      "metadata": {
        "id": "m4IENa5nD_JY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eefb9c7-2def-4ae8-a1c2-5138e39f03c8"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch[0].device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cwi6fRabpMyM",
        "outputId": "f69a18cc-ece9-4576-cd0b-8978218411f2"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# batch[1]\n",
        "\n",
        "\n",
        "# targets = [\n",
        "#     {\n",
        "#         'boxes': torch.tensor([[20, 30, 100, 120], [50, 60, 150, 140]]),\n",
        "#         'labels': torch.tensor([1, 2]),\n",
        "#     },\n",
        "#     {\n",
        "#         'boxes': torch.tensor([[30, 40, 120, 130]]),\n",
        "#         'labels': torch.tensor([1]),\n",
        "#     }\n",
        "# ]"
      ],
      "metadata": {
        "id": "Ceds_e_Zv8Gi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(batch)\n",
        "# batch[1]"
      ],
      "metadata": {
        "id": "knG4Qu0cJDGj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch[0].shape\n",
        "# aaa[0]"
      ],
      "metadata": {
        "id": "_BGXX1R0vWOY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for batch in train_loader:\n",
        "#     image,bbox = batch[0],batch[1]\n",
        "#     print(bbox)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Kp59HYpbhfEC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = torchvision.models.resnet50()\n",
        "# model = torch.nn.Sequential(*list(torchvision.models.resnet101(pretrained=True).children())[:-3])\n",
        "# model\n",
        "# model(batch[0]).shape"
      ],
      "metadata": {
        "id": "537yXwkpQWol"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torchvision.models\n",
        "# from torchvision.models.detection.rpn import RegionProposalNetwork\n",
        "# from torchvision.models.detection.rpn import AnchorGenerator\n",
        "# from torchvision.models.detection.rpn import RPNHead\n",
        "# from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "\n",
        "\n",
        "\n",
        "# # Define the RPN model\n",
        "# rpn = RegionProposalNetwork(\n",
        "#     anchor_generator=AnchorGenerator(sizes=((32,), (64,), (128,))),\n",
        "#     head=RPNHead(256,9),\n",
        "#     fg_iou_thresh=0.7,\n",
        "#     bg_iou_thresh=0.3,\n",
        "#     batch_size_per_image=256,\n",
        "#     positive_fraction=0.5,\n",
        "#     pre_nms_top_n=1000,\n",
        "#     post_nms_top_n=1000,\n",
        "#     nms_thresh=0.7,\n",
        "# )\n",
        "\n",
        "\n",
        "# # Define the loss function\n",
        "# criterion = torch.nn.BCELoss()\n",
        "\n",
        "# # Define the optimizer\n",
        "# optimizer = torch.optim.Adam(rpn.parameters(), lr=0.001)\n",
        "\n",
        "# # model.eval()\n",
        "\n",
        "# # Train the model\n",
        "# for epoch in range(10):\n",
        "#   for images, bboxes in train_loader:\n",
        "#     # Feed the images and bboxes to the model\n",
        "#     f = model(images)\n",
        "#     predictions = rpn(images,f)\n",
        "\n",
        "#     # Calculate the loss\n",
        "#     loss = criterion(predictions, bboxes)\n",
        "\n",
        "#     # Update the model parameters\n",
        "#     optimizer.zero_grad()\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     print(\"one step done!\")\n",
        "#     break\n",
        "\n",
        "# # Evaluate the model\n",
        "# # ...\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SZ3YEUc3E2OP"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "# from torchvision.models.detection.rpn import RegionProposalNetwork\n",
        "\n",
        "# # Load the ResNet model\n",
        "# resnet = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "# # Load the RPN model\n",
        "# rpn = RegionProposalNetwork(\n",
        "#     anchor_generator=AnchorGenerator(sizes=((32,), (64,), (128,))),\n",
        "#     head=RPNHead(256),\n",
        "#     fg_iou_thresh=0.7,\n",
        "#     bg_iou_thresh=0.3,\n",
        "#     batch_size_per_image=256,\n",
        "#     positive_fraction=0.5,\n",
        "#     pre_nms_top_n=1000,\n",
        "#     post_nms_top_n=1000,\n",
        "#     nms_thresh=0.7,\n",
        "# )\n",
        "\n",
        "# # Load the image\n",
        "# image = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "# # Extract the features from the image using ResNet\n",
        "# features = resnet.features(image)\n",
        "\n",
        "# # Feed the features to the RPN model\n",
        "# predictions = rpn(features)\n",
        "\n",
        "# # Process the predictions\n"
      ],
      "metadata": {
        "id": "JNGJefjcYf_l"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # #FASTRCNN\n",
        "\n",
        "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn()\n",
        "# # For training\n",
        "# images, boxes = torch.rand(8, 3, 600, 1200), torch.rand(8, 11, 4)\n",
        "# boxes[:, :, 2:4] = boxes[:, :, 0:2] + boxes[:, :, 2:4]\n",
        "# labels = torch.randint(1, 91, (8, 11))\n",
        "# images = list(image for image in images)\n",
        "# targets = []\n",
        "# for i in range(len(images)):\n",
        "#      d = {}\n",
        "#      d['boxes'] = boxes[i]\n",
        "#     #  d['labels'] = labels[i]\n",
        "#      targets.append(d)\n",
        "\n",
        "# output = model(images, targets)\n",
        "# # # >>> # For inference\n",
        "# # # >>> model.eval()\n",
        "# # # >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
        "# # # >>> predictions = model(x)\n",
        "# # # >>>\n",
        "# # # >>> # optionally, if you want to export the model to ONNX:\n",
        "# # # >>> torch.onnx.export(model, x, \"faster_rcnn.onnx\", opset_version = 11)"
      ],
      "metadata": {
        "id": "svDhG07AtfkJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# targets"
      ],
      "metadata": {
        "id": "hMdVdsmMzayC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# boxes.shape"
      ],
      "metadata": {
        "id": "e3B4InOtuyin"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# targets[0]"
      ],
      "metadata": {
        "id": "j-TN7R7DzH8U"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output"
      ],
      "metadata": {
        "id": "AbW95WMct-E2"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torchvision\n",
        "# from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "# # Load the pre-trained Faster R-CNN model\n",
        "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "# # Remove the classification and regression heads\n",
        "# model.roi_heads.box_predictor = None\n",
        "\n",
        "# # Create an RPN-only model by extracting the RPN module\n",
        "# rpn_model = model.rpn\n",
        "\n",
        "# # Set the RPN model to evaluation mode\n",
        "# rpn_model.train()\n",
        "\n",
        "# # Define an input tensor (image)\n",
        "# input_image = torch.randn(1, 3, 224, 224)  # Example input image\n",
        "\n",
        "# ground_truth_boxes = [torch.tensor([[20, 20, 100, 100], [50, 50, 150, 150]]),  # Example ground truth bounding boxes for image 1\n",
        "#                       torch.tensor([[30, 30, 120, 120], [60, 60, 140, 140]])]  # Example ground truth bounding boxes for image 2\n",
        "# labels = [torch.tensor([1, 1]), torch.tensor([1, 1])]\n",
        "# # Generate region proposals using the RPN\n",
        "# backbone_features = model.backbone(input_image)\n",
        "# proposals, _ = rpn_model(input_image,backbone_features,ground_truth_boxes)\n",
        "\n",
        "# print(proposals)"
      ],
      "metadata": {
        "id": "hwifQ1SNxT4C"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rpn_model"
      ],
      "metadata": {
        "id": "F6oqRCPCv9aO"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torchvision\n",
        "# from torchvision.models.detection.rpn import AnchorGenerator\n",
        "# from torchvision.models.detection.image_list import ImageList\n",
        "\n",
        "# # Load the pre-trained Faster R-CNN model\n",
        "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "# # Remove the classification and regression heads\n",
        "# model.roi_heads.box_predictor = None\n",
        "\n",
        "# # Create an RPN-only model by extracting the RPN module\n",
        "# rpn_model = model.rpn\n",
        "\n",
        "# # Set the RPN model to evaluation mode\n",
        "# rpn_model.eval()\n",
        "\n",
        "# # Define an input tensor (image)\n",
        "# input_image = torch.randn(2, 3, 224, 224)  # Example input image\n",
        "\n",
        "# # Wrap the input tensor in an ImageList object\n",
        "# image_list = ImageList(input_image, [(input_image.shape[-2], input_image.shape[-1])])\n",
        "\n",
        "# # Get features from the backbone network\n",
        "# backbone_features = model.backbone(image_list.tensors)\n",
        "\n",
        "# # Generate region proposals using the RPN\n",
        "# proposals, _ = rpn_model(image_list, backbone_features)\n",
        "\n",
        "# print(proposals)"
      ],
      "metadata": {
        "id": "rjJ3MAisRo7u"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.models.detection.image_list import ImageList\n",
        "\n",
        "\n",
        "#DONT TOUCH IT , IT WORKS!!\n",
        "\n",
        "\n",
        "# Load the pre-trained Faster R-CNN model\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "# Set the RPN model to training mode\n",
        "model.train()\n",
        "# Remove the classification and regression heads\n",
        "\n",
        "# model.roi_heads.box_predictor = None # CHECK THIS OUT!! double check !!!!!!!!!\n",
        "\n",
        "# Create an RPN-only model by extracting the RPN module\n",
        "# rpn_model = model.rpn.head()\n",
        "rpn_model = model.rpn\n",
        "#Non max score\n",
        "# model.rpn.score_thresh = 0.7 # i need to understands this more by watching non max\n",
        "\n",
        "# Set the RPN model to evaluation mode\n",
        "rpn_model.train()\n",
        "# Example input image\n",
        "input_image = batch[0]\n",
        "\n",
        "# Wrap the input tensor in an ImageList object\n",
        "image_list = ImageList(input_image, [(input_image.shape[-2], input_image.shape[-1])] * input_image.shape[0])\n",
        "\n",
        "# Get features from the backbone network\n",
        "backbone_features = model.backbone(image_list.tensors)\n",
        "\n",
        "# Generate region proposals using the RPN\n",
        "proposals, loss = rpn_model(image_list, backbone_features,batch[1])\n",
        "\n",
        "# print(proposals)\n",
        "loss"
      ],
      "metadata": {
        "id": "2miW8vbK2i0O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7787e9ed-2d12-4ba1-d53d-18e6525fc42a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
            "100%|██████████| 160M/160M [00:01<00:00, 103MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss_objectness': tensor(2.1227, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
              " 'loss_rpn_box_reg': tensor(1.0790, grad_fn=<DivBackward0>)}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4y1mm6pIH-ct"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# objectness[4].shape"
      ],
      "metadata": {
        "id": "iLsV9FKuIn-T"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "output = model(batch[0],batch[1])\n"
      ],
      "metadata": {
        "id": "GfDmHdG6tMKw"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output[5]['boxes'].shape"
      ],
      "metadata": {
        "id": "FJMtu1s6SMZW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecf546ff-ae44-4fe1-da16-010d2401a840"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.rpn.nms_thresh"
      ],
      "metadata": {
        "id": "K-Tcz19NGfFk"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch[1][7]['boxes']*NEWSIZE[0] # unnormalized\n",
        "\n",
        "# apply non max suppression to the proposed regions\n",
        "len(proposals[0])"
      ],
      "metadata": {
        "id": "S3opoVhSzVn-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5ee1388-c61e-4b1f-b969-3c3a4c51ae73"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "proposals[7].shape\n"
      ],
      "metadata": {
        "id": "MPGSYGSTITJ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89dfa80d-500a-48b9-9be6-87d66580b00a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2000, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "7YPaIQVv70fN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.rpn"
      ],
      "metadata": {
        "id": "l6lLE1tIF4KF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8db078c6-97f5-45fe-f483-408cf260c247"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RegionProposalNetwork(\n",
              "  (anchor_generator): AnchorGenerator()\n",
              "  (head): RPNHead(\n",
              "    (conv): Sequential(\n",
              "      (0): Conv2dNormActivation(\n",
              "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_list.tensors.device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkmUdJQHnqy1",
        "outputId": "f0c060ca-e2c2-4b35-8aa0-0d38e8dcfd72"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as opt\n",
        "\n",
        "rpn_model.train()\n",
        "params_to_optimize = []\n",
        "params_to_optimize += list(rpn_model.parameters())\n",
        "params_to_optimize += list(model.backbone.parameters())\n",
        "\n",
        "optimizer = opt.SGD(params_to_optimize,lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
        "EPOCHS = 1\n",
        "train_loss = []\n",
        "\n",
        "device = torch.device('cuda:0')\n",
        "backbone = model.backbone.to(device)\n",
        "rpn_model = rpn_model.to(device)\n",
        "for i in range(EPOCHS):\n",
        "    loss = 0\n",
        "    for i,batch in enumerate(train_loader):\n",
        "        print(i)\n",
        "        image, bbox = batch\n",
        "        image = image.to(device)\n",
        "        # image, bbox = image.to(device),bbox.to(device)\n",
        "        image_list = ImageList(image, [(image.shape[-2], image.shape[-1])] * image.shape[0]) # imgs,imgs_sizes*batch size\n",
        "        backbone_features = backbone((image_list.tensors).to(device))\n",
        "        proposals, losses = rpn_model(image_list, backbone_features,batch[1])\n",
        "        losses = losses[\"loss_objectness\"] + losses[\"loss_rpn_box_reg\"]\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "        loss += losses\n",
        "    train_loss.append(loss/len(train_loader))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "z_9Kk5zG73SV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "59e4e31e-8d78-4bb4-ba6c-b0b3e7ecc3b0"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n",
            "0\n",
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n",
            "1\n",
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n",
            "2\n",
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n",
            "3\n",
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n",
            "4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-17ea9770c9f7>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mimage_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# imgs,imgs_sizes*batch size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mbackbone_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrpn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackbone_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss_objectness\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss_rpn_box_reg\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;31m# note that we detach the deltas because Faster R-CNN do not backprop through\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;31m# the proposals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mproposals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_coder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_bbox_deltas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m         \u001b[0mproposals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_proposals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjectness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_anchors_per_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/detection/_utils.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, rel_codes, boxes)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbox_sum\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mrel_codes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrel_codes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mpred_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrel_codes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_boxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbox_sum\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mpred_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_boxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/detection/_utils.py\u001b[0m in \u001b[0;36mdecode_single\u001b[0;34m(self, rel_codes, boxes)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# Distance from center to box's corner.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mc_to_c_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred_ctr_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred_h\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpred_h\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0mc_to_c_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred_ctr_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred_w\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpred_w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "losses"
      ],
      "metadata": {
        "id": "gav0m2c9O_6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rpn_model.eval()\n",
        "# out = rpn_model(image_list,backbone_features)\n",
        "# on evaluation mode , we got 1000 preposal regions\n",
        "# we dont get losses\n",
        "results = []\n",
        "for i in range(len(proposals)):\n",
        "    boxes = torchvision.ops.nms(proposals[i],scores[i],0.1)\n",
        "    results.append(boxes)\n",
        "\n",
        "# nm = torchvision.ops.nms(proposals_tensors,scores_tensors,0.5)\n",
        "# nm\n",
        "results[0].shape\n"
      ],
      "metadata": {
        "id": "CymQCtIpwMdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bboxes = []\n",
        "for i in nm:\n",
        "    bboxes.append(out[0][0][i])"
      ],
      "metadata": {
        "id": "Zw8G9wYTBH4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Av1mXOj7wN6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "\n",
        "image = batch[0][0]\n",
        "boxes = bboxes\n",
        "plt.imshow(image.permute(1,2,0))\n",
        "# print(len(boxes))\n",
        "\n",
        "# Draw bounding boxes on the image\n",
        "for box in boxes:\n",
        "\n",
        "    x, y, x2, y2 = box\n",
        "    width = (x2-x)\n",
        "    height = (y2-y)\n",
        "\n",
        "    plt.gca().add_patch(matplotlib.patches.Rectangle((x,y),width,height,\n",
        "                    edgecolor='red',\n",
        "                    facecolor='none',\n",
        "                    lw=4))\n"
      ],
      "metadata": {
        "id": "c4NUncrSwkXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "proposals[0].shape"
      ],
      "metadata": {
        "id": "sxCjDjWop5MH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "image_list.image_sizes"
      ],
      "metadata": {
        "id": "_10g5xERcLn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.backbone(torch.randn(1,3,224,224))\n",
        "model.rpn.head.cls_logits(torch.randn(1,256,256,3)).shape"
      ],
      "metadata": {
        "id": "x0ISvJzsTciS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.backbone(torch.randn(1,3,224,224))\n",
        "# logits = []\n",
        "# bbox_reg = []\n",
        "#     for feature in x:\n",
        "#             t = model.backbone(feature)\n",
        "#             logits.append(self.cls_logits(t))\n",
        "#             bbox_reg.append(self.bbox_pred(t))"
      ],
      "metadata": {
        "id": "ojJlK61zaNXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trans = transforms.Compose([\n",
        "#    transforms.ToTensor(),\n",
        "# #    transforms.Resize((350,350)),\n",
        "# #    transforms.CenterCrop(224),\n",
        "#    transforms.Normalize(mean=[0.485, 0.456, 0.406], # ImageNet\n",
        "#                         std=[0.229, 0.224, 0.225]),\n",
        "# ])\n",
        "# dataset = CustomeDataset(path='/content/datadir/instances_val2017.json' , trans = trans)\n"
      ],
      "metadata": {
        "id": "eiV7Fx9IMGxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import cv2\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import matplotlib\n",
        "\n",
        "# image, boxes = (dataset[55][0]),dataset[55][1]\n",
        "# print(image.shape)\n",
        "# plt.imshow(image.permute(1,2,0))\n",
        "# # Draw bounding boxes on the image\n",
        "# for box in boxes:\n",
        "#     print(box)\n",
        "#     print(\"===============\")\n",
        "#     _x, _y, width, height = box\n",
        "#     print(width)\n",
        "#     x = _x * wScale\n",
        "#     y = _y * hScale\n",
        "#     w = width*wScale\n",
        "#     h = height * hScale\n",
        "#     color = (0, 255, 0)\n",
        "#     plt.gca().add_patch(matplotlib.patches.Rectangle((x,y),w,h,\n",
        "#                     edgecolor='red',\n",
        "#                     facecolor='none',\n",
        "#                     lw=4))\n",
        "#     # Display the image with bounding boxes\n",
        "# # cv2.imshow(\"Image with Bounding Boxes\", image)\n",
        "# # cv2.waitKey(0)\n",
        "# # cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "ox2FSTqtK4ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x\n",
        "# w/width"
      ],
      "metadata": {
        "id": "trF3-pQmTv4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image.shape[1]\n",
        "# height = 424\n",
        "# width = 640\n",
        "# wScale = NEWSIZE[0]/width\n",
        "# hScale = NEWSIZE[0]/height\n",
        "# image.shape[1]"
      ],
      "metadata": {
        "id": "uiLR_CR9L-ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "\n",
        "\n",
        "image, boxes = (dataset[51][0]),dataset[51][1]\n",
        "plt.imshow(image.permute(1,2,0))\n",
        "\n",
        "# Draw bounding boxes on the image\n",
        "for box in boxes:\n",
        "\n",
        "    x, y, x2, y2 = box\n",
        "    width = (x2-x)*NEWSIZE[1]\n",
        "    height = (y2-y)*NEWSIZE[0]\n",
        "\n",
        "    plt.gca().add_patch(matplotlib.patches.Rectangle((x*NEWSIZE[1],y*NEWSIZE[0]),width,height,\n",
        "                    edgecolor='red',\n",
        "                    facecolor='none',\n",
        "                    lw=4))\n"
      ],
      "metadata": {
        "id": "uB4uwPwNOm1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_,h,w = dataset2[8][0].shape\n"
      ],
      "metadata": {
        "id": "3X3bIOdmBUI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Center Crop Source Code"
      ],
      "metadata": {
        "id": "j0OxHr27obtL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def center_crop(img: Tensor, output_size: List[int]) -> Tensor:\n",
        "    \"\"\"Crops the given image at the center.\n",
        "    If the image is torch Tensor, it is expected\n",
        "    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions.\n",
        "    If image size is smaller than output size along any edge, image is padded with 0 and then center cropped.\n",
        "\n",
        "    Args:\n",
        "        img (PIL Image or Tensor): Image to be cropped.\n",
        "        output_size (sequence or int): (height, width) of the crop box. If int or sequence with single int,\n",
        "            it is used for both directions.\n",
        "\n",
        "    Returns:\n",
        "        PIL Image or Tensor: Cropped image.\n",
        "    \"\"\"\n",
        "    if not torch.jit.is_scripting() and not torch.jit.is_tracing():\n",
        "        _log_api_usage_once(center_crop)\n",
        "    if isinstance(output_size, numbers.Number):\n",
        "        output_size = (int(output_size), int(output_size))\n",
        "    elif isinstance(output_size, (tuple, list)) and len(output_size) == 1:\n",
        "        output_size = (output_size[0], output_size[0])\n",
        "\n",
        "    _, image_height, image_width = get_dimensions(img)\n",
        "    crop_height, crop_width = output_size\n",
        "\n",
        "    if crop_width > image_width or crop_height > image_height:\n",
        "        padding_ltrb = [\n",
        "            (crop_width - image_width) // 2 if crop_width > image_width else 0,\n",
        "            (crop_height - image_height) // 2 if crop_height > image_height else 0,\n",
        "            (crop_width - image_width + 1) // 2 if crop_width > image_width else 0,\n",
        "            (crop_height - image_height + 1) // 2 if crop_height > image_height else 0,\n",
        "        ]\n",
        "        img = pad(img, padding_ltrb, fill=0)  # PIL uses fill value 0\n",
        "        _, image_height, image_width = get_dimensions(img)\n",
        "        if crop_width == image_width and crop_height == image_height:\n",
        "            return img\n",
        "\n",
        "    crop_top = int(round((image_height - crop_height) / 2.0))\n",
        "    crop_left = int(round((image_width - crop_width) / 2.0))\n",
        "    return crop(img, crop_top, crop_left, crop_height, crop_width)\n",
        "\n"
      ],
      "metadata": {
        "id": "xTiRk3yjd4f6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RHLIDjLXCJ8U"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}